{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Questions\n",
        "\n",
        "1)What is a parameter?\n",
        " - In machine learning, a parameter is a variable that is internal to the model and whose value is learned from the training data. Parameters define the model's skill in making predictions. They are not set manually but are estimated or learned from the data during the training process. Examples of parameters include the weights and biases in neural networks or the coefficients in linear regression. The process of training a model involves finding the optimal values for these parameters that minimize the error between the model's predictions and the actual values in the data.\n",
        "\n",
        "\n",
        "2)What is correlation? What does negative correlation mean?\n",
        "  - Correlation describes the relationship between two variables. A negative correlation, also known as an inverse correlation, means that when one variable increases, the other decreases, and vice versa. Essentially, they move in opposite directions.\n",
        "\n",
        "3)Define Machine Learning. What are the main components in Machine Learning?\n",
        "  - Machine Learning (ML) is a subfield of Artificial Intelligence (AI) focused on enabling systems to learn from data without being explicitly programmed. It involves developing algorithms that allow computers to identify patterns, make predictions, and improve their performance over time through experience and exposure to more data.\n",
        "  - The main components of a Machine Learning system are:\n",
        "- Data:\n",
        "The raw material for learning, which can be labeled, unlabeled, or a combination of both. The quality and quantity of data significantly impact the performance of the ML model.\n",
        "- Model:\n",
        "The algorithm or set of algorithms that the system uses to learn from the data. Models can range from simple linear regression to complex neural networks.\n",
        "- Training:\n",
        "The process of feeding data to the model so it can learn the underlying patterns and relationships. This step involves adjusting the model's parameters to minimize errors and improve accuracy.\n",
        "- Evaluation:\n",
        "Assessing the model's performance on new, unseen data to ensure it can generalize well and make accurate predictions. Metrics like accuracy, precision, and recall are used for evaluation.\n",
        "- Prediction or Inference:\n",
        "Using the trained model to make predictions or decisions on new data instances. This is the ultimate goal of ML, enabling systems to automate tasks and solve problems.\n",
        "- Optimization:\n",
        "Refining the model by adjusting its parameters to enhance its performance, typically through techniques like gradient descent.\n",
        "Representation:\n",
        "How the data is structured and formatted, influencing how the model interprets and learns from it.\n",
        "\n",
        "\n",
        "\n",
        "4)How does loss value help in determining whether the model is good or not?\n",
        "  - A model's loss value, calculated using a loss function, indicates how well it performs. A lower loss value generally means the model is making better predictions, while a higher loss value suggests the model is making more errors. The loss function quantifies the difference between the model's predictions and the actual values (the \"ground truth\").\n",
        "\n",
        "\n",
        "5)What are continuous and categorical variables?\n",
        "  - In statistics, continuous variables represent numerical values that can take any value within a specified range, while categorical variables represent non-numerical data grouped into categories or groups. Continuous variables can be measured and have an infinite number of possible values, while categorical variables are qualitative and limited to a specific set of labels.\n",
        "\n",
        "\n",
        "6)How do we handle categorical variables in Machine Learning? What are the common techniques?     \n",
        "  - Categorical variables in machine learning are handled by converting them into a numerical format that algorithms can understand. Common techniques include one-hot encoding, label encoding, ordinal encoding, and target encoding. The best approach depends on the nature of the data and the algorithm being used.\n",
        "\n",
        "  - One-Hot Encoding:\n",
        "This creates binary columns for each unique category in the feature, effectively converting the categorical variable into a series of binary indicators.\n",
        "Example: If a feature \"color\" has categories \"red\", \"blue\", and \"green\", one-hot encoding would create three new columns: \"color\\_red\", \"color\\_blue\", and \"color\\_green\". Each row would have a 1 in the corresponding column if it belongs to that category and 0 otherwise.\n",
        "  - Label Encoding:\n",
        "This assigns a unique integer to each category, transforming the categorical variable into an ordinal variable.\n",
        "Example: The \"color\" feature would be encoded as \"red\": 0, \"blue\": 1, \"green\": 2.\n",
        "  - Ordinal Encoding:\n",
        "This preserves the order of the categories when assigning numerical values, particularly useful for variables with a clear hierarchy.\n",
        "Example: If \"size\" has categories \"small\", \"medium\", \"large\", and \"extra-large\", they could be encoded as 1, 2, 3, 4, respectively.\n",
        "  - Target Encoding:\n",
        "This replaces each category with the average of the target variable for that category.\n",
        "Example: If a feature \"customer\\_type\" has categories \"new\" and \"existing\", and the target variable is \"purchase\\_amount\", target encoding would replace \"new\" with the average purchase amount for new customers and \"existing\" with the average purchase amount for existing customers.\n",
        "\n",
        "\n",
        "7)What do you mean by training and testing a dataset?\n",
        "  - In machine learning, \"training\" and \"testing\" a dataset refer to the process of splitting a dataset into two parts: one to train the model and the other to evaluate its performance. The training dataset is used to teach the model to identify patterns and make predictions, while the testing dataset is used to assess how well the trained model generalizes to new, unseen data.\n",
        "\n",
        "8)What is sklearn.preprocessing?\n",
        "  - The sklearn.preprocessing module in scikit-learn provides a suite of functions and classes designed to transform raw data into a format more suitable for machine learning algorithms. It covers various techniques, including scaling, normalization, encoding, and imputation. These methods address common data issues such as differing scales, non-normal distributions, categorical variables, and missing values, ultimately improving model performance and accuracy.\n",
        "\n",
        "9)What is a Test set?\n",
        "  - A test set is a subset of data, separate from the training set, used to evaluate how well a model performs on unseen data after it has been trained. It simulates real-world data and helps assess the model's generalization ability, ensuring it can make accurate predictions on new inputs.  \n",
        "\n",
        "\n",
        "10)How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "  - Data splitting in Python, particularly for machine learning, typically involves dividing a dataset into training and testing sets, and sometimes a validation set. This process helps to evaluate the performance of a model on unseen data. The scikit-learn library provides the train_test_split function for this purpose.  \n",
        "\n",
        "\n",
        "11)Why do we have to perform EDA before fitting a model to the data?\n",
        "  - Exploratory Data Analysis (EDA) is crucial before fitting a model because it helps identify errors, understand data patterns, and prepare the data for modeling effectively. By performing EDA, data scientists can gain insights into the data's characteristics, distributions, and relationships, allowing them to make informed decisions about feature engineering, model selection, and overall model performance.    \n",
        "\n",
        "12)What is correlation?\n",
        "  - Correlation in machine learning is a statistical measure that expresses the extent to which two variables are linearly related, meaning they change together at a constant rate. It is a crucial tool for understanding relationships between features in a dataset and can provide insights for effective model building and interpretation. The correlation coefficient ranges from -1 to +1, where\n",
        "  - Close to +1 indicates a strong positive correlation (as one variable increases, the other tends to increase).\n",
        "  - Close to -1 indicates a strong negative correlation (as one variable increases, the other tends to decrease).\n",
        "  - Close to 0 indicates no linear correlation.\n",
        "\n",
        "\n",
        "13)What does negative correlation mean?\n",
        "  - A negative correlation, also known as an inverse correlation, means that as one variable increases, the other variable decreases, and vice versa. This is a relationship where the variables move in opposite directions.   \n",
        "\n",
        "\n",
        "14)How can you find correlation between variables in Python?\n",
        "    \n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [2, 4, 5, 4, 5], 'C': [5, 4, 3, 2, 1]})\n",
        "    correlation_matrix = df.corr()\n",
        "    print(correlation_matrix)  \n",
        "\n",
        "\n",
        "15)What is causation? Explain difference between correlation and causation with an example.\n",
        "   - Causation means one event directly causes another to happen, while correlation indicates a relationship between two events, where one doesn't necessarily cause the other. For example, hitting a billiard ball with a cue stick causes the ball to move, but eating ice cream and getting sunburned are correlated because both occur more often in sunny weather, not because eating ice cream causes sunburns.\n",
        "\n",
        "\n",
        "16)What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "   - In machine learning and deep learning, an optimizer is an algorithm used to adjust the parameters of a model, such as weights and biases, to minimize the loss function during training. The loss function measures the difference between the predicted output and the actual output, and the optimizer's goal is to find the set of parameters that minimizes this difference.    \n",
        "\n",
        "\n",
        "17)What is sklearn.linear_model?\n",
        "  - sklearn.linear_model is a module in the scikit-learn (sklearn) library in Python that implements various linear models for regression and classification tasks. Linear models predict the target variable using a linear combination of the input features. These models are widely used due to their simplicity, interpretability, and efficiency, serving as building blocks for more complex algorithms.    \n",
        "\n",
        "\n",
        "18)What does model.fit() do? What arguments must be given?\n",
        "  - The model.fit() function in machine learning frameworks like TensorFlow and Keras trains a model on a given dataset. It iteratively adjusts the model's internal parameters (weights and biases) to minimize the loss function, effectively learning the patterns in the data.  \n",
        "\n",
        "\n",
        "19)What does model.predict() do? What arguments must be given?\n",
        "  - The model.predict() method in machine learning is used to generate predictions on new, unseen data after the model has been trained. It takes one primary argument.\n",
        "\n",
        "\n",
        "20)What are continuous and categorical variables?\n",
        "  - In statistics, continuous variables represent numerical values that can take any value within a specified range, while categorical variables represent non-numerical data grouped into categories or groups. Continuous variables can be measured and have an infinite number of possible values, while categorical variables are qualitative and limited to a specific set of labels.   \n",
        "\n",
        "\n",
        "21)What is feature scaling? How does it help in Machine Learning?\n",
        "  - Feature scaling in machine learning is the process of normalizing or standardizing the range of independent variables in a dataset. It helps by ensuring that all features contribute equally to the model, preventing those with larger values from dominating the model's performance.   \n",
        "\n",
        "\n",
        "22)How do we perform scaling in Python?\n",
        "  - Scaling in Python involves adjusting the range of data values. This is commonly performed as a preprocessing step in machine learning to ensure that all features contribute equally to the model and prevent features with larger values from dominating. Here's how scaling can be done using the scikit-learn library\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)  \n",
        "     \n",
        "\n",
        "23)What is sklearn.preprocessing?\n",
        "  - The sklearn.preprocessing module in scikit-learn provides a suite of functions and classes to transform raw data into a format suitable for machine learning algorithms. It encompasses various techniques for feature scaling, normalization, encoding, and data transformation. Preprocessing is a crucial step in the machine learning workflow as it ensures data consistency, improves model accuracy and efficiency, and handles issues like missing values and outliers.     \n",
        "\n",
        "\n",
        "24)How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('your_data.csv')\n",
        "\n",
        "X = data.drop('target_variable', axis=1)\n",
        "\n",
        "y = data['target_variable']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
        "\n",
        "\n",
        "\n",
        "25)Explain data encoding?\n",
        "  - Data encoding is the process of converting data from one format to another, often to make it more suitable for storage, transmission, or processing. It involves transforming information into a specific code or format, ensuring it can be read and interpreted by a computer or other system. This process is crucial for various applications, including ensuring data integrity, security, and compatibility between different systems."
      ],
      "metadata": {
        "id": "HUtRBSsJJ-JO"
      }
    }
  ]
}